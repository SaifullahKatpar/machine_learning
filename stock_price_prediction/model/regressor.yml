nn:
        loss: mse
        epochs: 350
        validation_split: 0.00
        learning_rate: 0.02
        metric: 
                - mean_squared_error
        optimizer: Adam
        layers:
                - layer_1:
                        activation: relu
                        dropout: 0.01
                        nuerons: 40
                - BatchNormalization
                - layer_2:
                        activation: relu
                        dropout: 0.01
                        nuerons: 40
                - layer_3:
                        activation: relu
                        dropout: 0.01
                        nuerons: 40
                - BatchNormalization
                - layer_4:
                        activation: relu
                        dropout: 0.01
                        nuerons: 40
                - BatchNormalization
                - layer_5:
                        activation: relu
                        dropout: 0.01
                        nuerons: 40
                - layer_6:
                        activation: linear
                        dropout: 0.00
                        nuerons: 1

